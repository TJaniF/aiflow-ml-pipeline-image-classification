"""
### DAG testing a binary HuggingFace image classifier using a custom operator

This DAG loads testing images from an S3 bucket and tests a HuggingFace
binary image classification model with them. 
The TestHuggingFaceBinaryImageClassifierOperator is a custom operator located in
the include folder of this ML pipeline example repository.
In the ML pipeline repository this DAG will test the fine-tuned model.
"""

from airflow import Dataset as AirflowDataset
from airflow.decorators import dag, task
from astro.sql import get_value_list
from airflow.operators.empty import EmptyOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.slack.notifications.slack_notifier import SlackNotifier
import pendulum
import os
import shutil


# import utility functions and variables from local files
from include.custom_operators.hugging_face import (
    TestHuggingFaceBinaryImageClassifierOperator,
)
from include.custom_operators.utils.utils import (
    standard_transform_function,
    write_all_model_metrics_to_duckdb,
)
from include.config_variables import (
    FINE_TUNED_MODEL_PATHS,
    SLACK_CHANNEL,
    SLACK_CONNECTION_ID,
    SLACK_MESSAGE,
    DB_CONN_ID,
    AWS_CONN_ID,
    TEST_DATA_TABLE_NAME,
    DUCKDB_PATH,
    DUCKDB_POOL_NAME,
    LOCAL_TEMP_TEST_FOLDER,
    RESULTS_TABLE_NAME,
)


@dag(
    start_date=pendulum.datetime(2023, 1, 1),
    schedule=[AirflowDataset("new_model_trained")],
    catchup=False,
)
def test_fine_tuned_model():
    start = EmptyOperator(task_id="start")
    end = EmptyOperator(task_id="end")

    # get list of images keys from the test table in DuckDB
    get_image_s3_keys_from_duckdb = get_value_list(
        task_id="get_image_s3_keys_from_duckdb",
        sql=f"SELECT image_s3_key FROM {TEST_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    # get corresponding labels from the test table in DuckDB
    get_labels_from_duckdb = get_value_list(
        task_id="get_labels_from_duckdb",
        sql=f"SELECT label FROM {TEST_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    # load test images from S3 into a temporary folder
    @task
    def load_test_images(keys):
        hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        images = []
        for key in keys:
            print(key)
            image = hook.download_file(
                key=key,
                preserve_file_name=True,
                local_path=LOCAL_TEMP_TEST_FOLDER,
                use_autogenerated_subdir=False,
            )
            images.append(image)

        return images

    local_images_filepaths = load_test_images(
        get_image_s3_keys_from_duckdb.map(lambda x: x[0])
    )

    # load the latest fine tuned model from local storage
    @task
    def get_latest_fine_tuned_model(fine_tuned_models_folder):
        models_dir = fine_tuned_models_folder
        models = []
        for dir in os.listdir(models_dir):
            models.append({"folder_name": dir, "timestamp": pendulum.parse(dir)})
        if not models:
            return None
        return (
            f"{fine_tuned_models_folder}/"
            + sorted(models, key=lambda m: m["timestamp"], reverse=True)[0][
                "folder_name"
            ]
            + "/"
        )

    # test the fine-tuned model
    test_classifier = TestHuggingFaceBinaryImageClassifierOperator(
        task_id="test_classifier",
        model_name=get_latest_fine_tuned_model(
            fine_tuned_models_folder=FINE_TUNED_MODEL_PATHS
        ),
        local_images_filepaths=local_images_filepaths,
        labels=get_labels_from_duckdb.map(lambda x: x[0]),
        test_transform_function=standard_transform_function,
        batch_size=500,
        shuffle=False,
        # if this task is successful send a slack notification
        on_success_callback=SlackNotifier(
            slack_conn_id=SLACK_CONNECTION_ID,
            text=SLACK_MESSAGE,
            channel=SLACK_CHANNEL,
        ),
        outlets=[AirflowDataset("new_model_tested")],
    )

    # delete temporary files
    @task
    def delete_local_test_files(folder_path):
        shutil.rmtree(folder_path)

    # write model results into a results table in duckdb
    @task(pool=DUCKDB_POOL_NAME)
    def write_model_results_to_duckdb(db_path, table_name, **context):
        write_all_model_metrics_to_duckdb(db_path, table_name, **context)

    (
        start
        >> [
            local_images_filepaths,
            get_labels_from_duckdb,
            get_image_s3_keys_from_duckdb,
        ]
    )

    (
        test_classifier
        >> [
            delete_local_test_files(LOCAL_TEMP_TEST_FOLDER),
            write_model_results_to_duckdb(
                db_path=DUCKDB_PATH,
                table_name=RESULTS_TABLE_NAME,
            ),
        ]
        >> end
    )


test_fine_tuned_model()
