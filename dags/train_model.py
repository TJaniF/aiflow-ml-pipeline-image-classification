"""
### DAG training a HuggingFace Image Classifier using a custom operator

This DAG loads training images from an S3 bucket and fine-tunes a HuggingFace
binary image classification model with them. 
The FineTuneHuggingFaceBinaryImageClassifierOperator is a custom operator located in
the include folder of this ML pipeline example repository.
"""

from airflow import Dataset as AirflowDataset
from airflow.decorators import dag, task
from astro.sql import get_value_list
from airflow.operators.empty import EmptyOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from pendulum import datetime
import shutil

from include.custom_operators.hugging_face import (
    FineTuneHuggingFaceBinaryImageClassifierOperator,
)
from include.custom_operators.utils.utils import standard_transform_function
from include.config_variables import (
    FINE_TUNED_MODEL_PATHS,
    DB_CONN_ID,
    DUCKDB_PATH,
    DUCKDB_POOL_NAME,
    TRAIN_DATA_TABLE_NAME,
    AWS_CONN_ID,
    LOCAL_TEMP_TRAIN_FOLDER,
    BASE_MODEL_NAME
)


@dag(
    start_date=datetime(2023, 1, 1),
    schedule=[AirflowDataset(f"duckdb://{DUCKDB_PATH}/{TRAIN_DATA_TABLE_NAME}")],
    catchup=False,
)
def train_model():
    start = EmptyOperator(task_id="start")
    end = EmptyOperator(task_id="end")

    # retrieve references to the images in the train set from DuckDB
    get_image_s3_keys_from_duckdb = get_value_list(
        task_id="get_image_s3_keys_from_duckdb",
        sql=f"SELECT image_s3_key FROM {TRAIN_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    # retrieve labels from DuckDB
    get_labels_from_duckdb = get_value_list(
        task_id="get_labels_from_duckdb",
        sql=f"SELECT label FROM {TRAIN_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    # load training images from S3
    @task
    def load_training_images(keys):
        hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        images = []
        for key in keys:
            image = hook.download_file(
                key=key,
                preserve_file_name=True,
                local_path=LOCAL_TEMP_TRAIN_FOLDER,
                use_autogenerated_subdir=False,
            )
            images.append(image)
            print(f"Successfully loaded image from {key}!")
        return images

    local_images_filepaths = load_training_images(
        get_image_s3_keys_from_duckdb.map(lambda x: x[0])
    )

    # fine tune resnet
    train_classifier = FineTuneHuggingFaceBinaryImageClassifierOperator(
        task_id="train_classifier",
        model_name=BASE_MODEL_NAME,
        local_images_filepaths=local_images_filepaths,
        labels=get_labels_from_duckdb.map(lambda x: x[0]),
        learning_rate=0.0023,
        model_save_dir=FINE_TUNED_MODEL_PATHS + "/{{ ts }}/",
        train_transform_function=standard_transform_function,
        batch_size=32,
        num_epochs=10,
        shuffle=True,
        outlets=[AirflowDataset("new_model_trained")],
    )

    # delete files from temporary local folder
    @task
    def delete_local_train_files(folder_path):
        shutil.rmtree(folder_path)

    # set Airflow dependencies
    (
        start
        >> [
            local_images_filepaths,
            get_labels_from_duckdb,
            get_image_s3_keys_from_duckdb,
        ]
    )

    train_classifier >> delete_local_train_files(LOCAL_TEMP_TRAIN_FOLDER) >> end


train_model()
