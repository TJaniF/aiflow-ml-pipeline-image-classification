"""
### TITLE

DESCRIPTION
"""

from airflow import Dataset as AirflowDataset
from airflow.decorators import dag, task
from astro import sql as aql
from astro.sql import get_value_list
from astro.files import get_file_list
from astro.sql.table import Table
from airflow.operators.empty import EmptyOperator
from airflow.operators.bash import BashOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

import pandas as pd
from pendulum import datetime
import os
import logging
import requests
import numpy as np
from PIL import Image
import duckdb
import json
import pickle
import shutil
import torch
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset as TorchDataset
from transformers import ViTFeatureExtractor, ResNetForImageClassification

from include.custom_operators.hugging_face import (
    TrainHuggingFaceImageClassifierOperator,
    transform_function,
)

task_logger = logging.getLogger("airflow.task")

TRAIN_FILEPATH = "include/train"
TEST_FILEPATH = "include/test"
FILESYSTEM_CONN_ID = "local_file_default"
DB_CONN_ID = "duckdb_default"
REPORT_TABLE_NAME = "reporting_table"
TEST_TABLE_NAME = "test_table"

S3_BUCKET_NAME = "myexamplebucketone"
S3_IN_FOLDER_NAME = "in_train_data"
S3_TRAIN_FOLDER_NAME = "train_data"
AWS_CONN_ID = "aws_default"
IMAGE_FORMAT = ".jpeg"
TRAIN_DATA_TABLE_NAME = "train_data"
DUCKDB_PATH = "include/duckdb_database"
DUCKDB_POOL_NAME = "duckdb_pool"

LABEL_TO_INT_MAP = {"glioma": 0, "meningioma": 1}
LOCAL_TEMP_TRAIN_FOLDER = "include/train"


@dag(
    start_date=datetime(2023, 1, 1),
    schedule=[AirflowDataset(f"duckdb://{DUCKDB_PATH}/{TRAIN_DATA_TABLE_NAME}")],
    catchup=False,
)
def train_model():
    start = EmptyOperator(task_id="start")
    end = EmptyOperator(task_id="end")

    get_image_s3_keys_from_duckdb = get_value_list(
        task_id="get_image_s3_keys_from_duckdb",
        sql=f"SELECT image_s3_key FROM {TRAIN_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    get_labels_from_duckdb = get_value_list(
        task_id="get_labels_from_duckdb",
        sql=f"SELECT label FROM {TRAIN_DATA_TABLE_NAME};",
        conn_id=DB_CONN_ID,
        pool=DUCKDB_POOL_NAME,
    )

    @task
    def load_training_images(keys):
        hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        images = []
        for key in keys:
            print(key)
            image = hook.download_file(
                key=key,
                preserve_file_name=True,
                local_path=LOCAL_TEMP_TRAIN_FOLDER,
                use_autogenerated_subdir=False,
            )
            images.append(image)

        return images

    local_images_filepaths = load_training_images(
        get_image_s3_keys_from_duckdb.map(lambda x: x[0])
    )

    train_classifier = TrainHuggingFaceImageClassifierOperator(
        task_id="train_classifier",
        model_name="microsoft/resnet-50", # find newer one?
        criterion=torch.nn.CrossEntropyLoss(),  # binary entropy loss!
        optimizer=torch.optim.Adam,
        local_images_filepaths=local_images_filepaths,
        labels=get_labels_from_duckdb.map(lambda x: x[0]),
        num_classes=2,
        model_save_dir="include/pretrained_models/{{ ts }}/",
        train_transform_function=transform_function,
        batch_size=32,
        num_epochs=10,
        shuffle=True,
        outlets=[AirflowDataset("new_model_trained")]
    )

    @task
    def delete_local_train_files(folder_path):
        shutil.rmtree(folder_path)

    (
        start
        >> [
            local_images_filepaths,
            get_labels_from_duckdb,
            get_image_s3_keys_from_duckdb,
        ]
    )

    train_classifier >> delete_local_train_files(LOCAL_TEMP_TRAIN_FOLDER) >> end


train_model()
